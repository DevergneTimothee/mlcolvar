{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbonati@iit.local/software/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastTensorDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1],\n",
      "        [2]]), tensor([[1],\n",
      "        [4]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset,Subset,TensorDataset\n",
    "\n",
    "class FastTensorDataLoader:\n",
    "    \"\"\"\n",
    "    A DataLoader-like object for a set of tensors.\n",
    "    \n",
    "    It is much faster than TensorDataset + DataLoader because dataloader grabs individual indices of the dataset and calls cat (slow).\n",
    "\n",
    "    Notes\n",
    "    =====\n",
    "\n",
    "    Adapted from https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6. \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, batch_size=0, shuffle=False):\n",
    "        \"\"\"Initialize a FastTensorDataLoader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tensors : list of tensors or torch.Dataset or torch.Subset or list of torch.Subset object containing a tensors object\n",
    "            tensors to store. Must have the same length @ dim 0.\n",
    "        batch_size : int, optional\n",
    "            batch size, by default 0 (==single batch)\n",
    "        shuffle : bool, optional\n",
    "            if True, shuffle the data *in-place* whenever an\n",
    "            iterator is created out of this object, by default False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        FastTensorDataLoader\n",
    "            dataloader-like object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # check input type\n",
    "        if isinstance(tensors,Subset):\n",
    "            tensors = [ tensors.dataset.tensors[i][tensors.indices] for i in range(len(tensors.dataset.tensors)) ]\n",
    "        elif isinstance(tensors,Dataset):\n",
    "            tensors = [ tensors.tensors[i] for i in range(len(tensors.tensors)) ]\n",
    "        # check for input type list of Subset, and create a list of tensors\n",
    "        elif (isinstance(tensors,list) and isinstance(tensors[0],Subset) ):\n",
    "            new_tensors = []\n",
    "            tensor = torch.Tensor()\n",
    "            for j in range( len( tensors[0].dataset.tensors ) ):\n",
    "                for i in range( len( tensors ) ):\n",
    "                    if i == 0:\n",
    "                        tensor = tensors[i].dataset.tensors[j][tensors[i].indices]\n",
    "                    else:    \n",
    "                        tensor = torch.cat( (tensor, tensors[i].dataset.tensors[j][tensors[i].indices]), 0 )\n",
    "                new_tensors.append(tensor)\n",
    "            tensors = new_tensors\n",
    "\n",
    "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
    "        self.tensors = tensors\n",
    "\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size if batch_size > 0 else self.dataset_len\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Calculate # batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder > 0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(self.dataset_len)\n",
    "        else:\n",
    "            self.indices = None\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        if self.indices is not None:\n",
    "            indices = self.indices[self.i:self.i+self.batch_size]\n",
    "            batch = tuple(torch.index_select(t, 0, indices) for t in self.tensors)\n",
    "        else:\n",
    "            batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
    "        self.i += self.batch_size\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "def test_fasttensordataloader(): \n",
    "    X = torch.arange(1,11).unsqueeze(1)\n",
    "    y = X**2\n",
    "    dataloader = FastTensorDataLoader([X,y],batch_size=2)\n",
    "    print(next(iter(dataloader)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_fasttensordataloader()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util for computing running ave/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Running average +- std: 24.5 +- 14.57737922668457>\n",
      "<Running average +- std: 49.49495315551758 +- 29.15909194946289>\n",
      "<Running average +- std: tensor([22.5000, 23.5000, 24.5000, 25.5000, 26.5000]) +- tensor([15.1383, 15.1383, 15.1383, 15.1383, 15.1383])>\n"
     ]
    }
   ],
   "source": [
    "class RunningAverageStd(object):\n",
    "    \"\"\"\n",
    "    Implements Welford's algorithm for computing a running mean\n",
    "    and standard deviation as described at: \n",
    "        http://www.johndcook.com/standard_deviation.html\n",
    "\n",
    "    Adapted for pytorch from: https://gist.github.com/alexalemi/2151722\n",
    "\n",
    "    can take single values or iterables\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean : \n",
    "        returns the mean\n",
    "    std : int\n",
    "        returns the std\n",
    "    range: \n",
    "        alias for std\n",
    "    meanfull: \n",
    "        returns the mean and std of the mean\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lst=None):\n",
    "        self.k = 0\n",
    "        self.M = None\n",
    "        self.S = None\n",
    "        \n",
    "        self.__call__(lst)\n",
    "    \n",
    "    def update(self,x):\n",
    "        if x is None:\n",
    "            return\n",
    "        self.k += 1\n",
    "        if self.M is not None:\n",
    "            newM = self.M + (x - self.M)*1./self.k\n",
    "            newS = self.S + (x - self.M)*(x - newM)\n",
    "        else: \n",
    "            newM = x\n",
    "            newS = x * 0.\n",
    "        self.M, self.S = newM, newS\n",
    "\n",
    "    def consume(self,lst):\n",
    "        lst = iter(lst)\n",
    "        for x in lst:\n",
    "            self.update(x)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.k = 0\n",
    "        self.M = None\n",
    "        self.S = None\n",
    "\n",
    "    def __call__(self,x):\n",
    "        if hasattr(x,\"__iter__\"):\n",
    "            self.consume(x)\n",
    "        else:\n",
    "            self.update(x)\n",
    "            \n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.M\n",
    "    @property\n",
    "    def meanfull(self):\n",
    "        return self.mean, self.std/torch.sqrt(self.k)\n",
    "    @property\n",
    "    def std(self):\n",
    "        if self.k==0:\n",
    "            return self.S\n",
    "        if self.k==1:\n",
    "            return 0\n",
    "        return torch.sqrt(self.S/(self.k-1))\n",
    "    @property\n",
    "    def range(self):\n",
    "        return self.std\n",
    "    def __repr__(self):\n",
    "        return f\"<Running average +- std: {self.mean} +- {self.std}>\"\n",
    "\n",
    "def test_running_average():\n",
    "\n",
    "    foo = RunningAverageStd()\n",
    "    foo(torch.arange(0, 50 ))\n",
    "    print(foo)\n",
    "    foo(torch.arange(51,100))\n",
    "    print(foo)\n",
    "    assert foo.mean.ndim == 0\n",
    "\n",
    "    foo = RunningAverageStd()\n",
    "    foo(torch.arange(0, 50).reshape(10,5))\n",
    "    print(foo)\n",
    "    assert (foo.mean.ndim == 1 ) and ( foo.std.shape[0] == 5)\n",
    "    foo.reset()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_running_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "<Running mean and range : 24.5, 24.5 (min and max: [0,49])>\n",
      "<Running mean and range : 49.5, 49.5 (min and max: [0,99])>\n",
      "-------------\n",
      "<Running mean and range : tensor([22.5000, 23.5000, 24.5000, 25.5000, 26.5000]), tensor([22.5000, 22.5000, 22.5000, 22.5000, 22.5000]) (min and max: [tensor([0, 1, 2, 3, 4]),tensor([45, 46, 47, 48, 49])])>\n",
      "-------------\n",
      "<Running mean and range : tensor([ 5.5000, 51.0000]), tensor([ 4.5000, 49.0000]) (min and max: [tensor([1., 2.]),tensor([ 10., 100.])])>\n"
     ]
    }
   ],
   "source": [
    "class RunningMinMax(object):\n",
    "    \"\"\"\n",
    "    Running calculation of min and max.\n",
    "\n",
    "    can take single values or iterables\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    min : \n",
    "        returns the min\n",
    "    max : int\n",
    "        returns the max\n",
    "    mean: \n",
    "        returns the mean (min+max)/2\n",
    "    range: \n",
    "        returns the range (max-min)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lst=None):\n",
    "        self.k = 0 \n",
    "        self.Min = None\n",
    "        self.Max = None\n",
    "        self.M = None\n",
    "        self.S = None\n",
    "        \n",
    "        self.dim = 0\n",
    "        self.__call__(lst)\n",
    "    \n",
    "    def update(self,x):\n",
    "        if x is None:\n",
    "            return\n",
    "        self.k += 1\n",
    "        if self.M is not None:\n",
    "            min_stack = torch.stack((torch.min(x,self.dim)[0], self.Min )) \n",
    "            newMin = torch.min(min_stack,dim=self.dim)[0]\n",
    "            max_stack = torch.stack((torch.max(x,self.dim)[0], self.Max )) \n",
    "            newMax = torch.max(max_stack,dim=self.dim)[0]\n",
    "        else: \n",
    "            newMin = torch.min(x,dim=self.dim).values\n",
    "            newMax = torch.max(x,dim=self.dim).values\n",
    "\n",
    "        self.Min, self.Max = newMin, newMax\n",
    "        self.M = (self.Max + self.Min) / 2.0\n",
    "        self.S = (self.Max - self.Min) / 2.0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.k = 0\n",
    "        self.Min = None\n",
    "        self.Max = None\n",
    "        self.M = None\n",
    "        self.S = None\n",
    "\n",
    "    def __call__(self,x):\n",
    "        if (x is not None) and (x.ndim == 0):\n",
    "            x = x.unsqueeze(0) \n",
    "        self.update(x)\n",
    "            \n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.M\n",
    "    @property\n",
    "    def range(self):\n",
    "        return self.S\n",
    "    def __repr__(self):\n",
    "        return f\"<Running mean and range : {self.mean}, {self.range} (min and max: [{self.Min},{self.Max}])>\"\n",
    "\n",
    "def test_running_min_max():\n",
    "\n",
    "    print('-------------')\n",
    "    foo = RunningMinMax()\n",
    "    foo(torch.arange(0, 50 ))\n",
    "    print(foo)\n",
    "    foo(torch.arange(51,100))\n",
    "    print(foo)\n",
    "    assert foo.mean.ndim == 0\n",
    "    print('-------------')\n",
    "    foo = RunningMinMax()\n",
    "    foo(torch.arange(0, 50).reshape(10,5))\n",
    "    print(foo)\n",
    "    assert (foo.mean.ndim == 1 ) and ( foo.range.shape[0] == 5)\n",
    "    foo.reset()\n",
    "    print('-------------')\n",
    "    foo = RunningMinMax()\n",
    "    X = torch.Tensor([[1,100,],[10,2],[5,50]])\n",
    "    foo(X)\n",
    "    print(foo)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_running_min_max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (nn): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([0.1120], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self, layers, activation=\"relu\", **kwargs \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Define a simple neural network given the list of layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layers : list\n",
    "            Number of neurons per layer\n",
    "        activation : string\n",
    "            Activation function (relu, tanh, elu, linear)\n",
    "        **kwargs:\n",
    "            Optional arguments passed to torch base class\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # get activation function\n",
    "        activ = None\n",
    "        if activation == \"relu\":\n",
    "            activ = torch.nn.ReLU(True)\n",
    "        elif activation == \"elu\":\n",
    "            activ = torch.nn.ELU(True)\n",
    "        elif activation == \"tanh\":\n",
    "            activ = torch.nn.Tanh()\n",
    "        elif activation == \"linear\":\n",
    "            print(\"WARNING: no activation selected\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unknown activation. options: 'relu','elu','tanh','linear'. \"\n",
    "            )\n",
    "\n",
    "        # Create architecture\n",
    "        if not isinstance(layers[0],int):\n",
    "            raise TypeError('layers should be a list-type of integers.')\n",
    "        \n",
    "        modules = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            if i < len(layers) - 2:\n",
    "                modules.append(torch.nn.Linear(layers[i], layers[i + 1]))\n",
    "                if activ is not None:\n",
    "                    modules.append(activ)\n",
    "            else:\n",
    "                modules.append(torch.nn.Linear(layers[i], layers[i + 1]))\n",
    "\n",
    "        # store model and attributes\n",
    "        self.nn     = torch.nn.Sequential(*modules)\n",
    "        self.n_in   = layers[0]\n",
    "        self.n_out  = layers[-1]\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> (torch.tensor):\n",
    "        return self.nn(x)\n",
    "\n",
    "def test_feedforward():\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    n_in = 2\n",
    "    layers = [2,10,1]\n",
    "    model = FeedForward(layers,activation='relu')\n",
    "    \n",
    "    print(model)\n",
    "\n",
    "    X = torch.zeros(n_in)\n",
    "    print(model(X))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_feedforward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorator tests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import functools\n",
    "import inspect\n",
    "\n",
    "def logger(f):\n",
    "    @functools.wraps(f)\n",
    "    def wrapper(self,*args, **kwargs):\n",
    "        print(f.__name__)\n",
    "        return f(self,*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def apply_hooks(f):\n",
    "    @functools.wraps(f)\n",
    "    def wrapper(self,*args, **kwargs):\n",
    "        # loop over all saved hooks in class\n",
    "        for hook,funcs in self.hooks.items():\n",
    "            # check if the current function name matches a saved hook\n",
    "            # in case execute all the associated funcs\n",
    "            if hook == f.__name__:\n",
    "                # if it is a string convert to iterable\n",
    "                if isinstance(funcs, str):\n",
    "                    funcs = [ funcs ] \n",
    "                for func in funcs:\n",
    "                    getattr(self,func)()\n",
    "        return f(self,*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\"\"\"Iterate through all methods in a class and apply decorator.\n",
    "DOES NOT WORK FOR INHERITED METHODS.\"\"\"\n",
    "def decorate_all_methods_v1(decorator, exclude=None):\n",
    "    if exclude is None:\n",
    "        exclude = []\n",
    "\n",
    "    def decorate(cls):\n",
    "        for attr in cls.__dict__:\n",
    "            if callable(getattr(cls, attr)) and attr not in exclude:\n",
    "                setattr(cls, attr, decorator(getattr(cls, attr)))\n",
    "        return cls\n",
    "\n",
    "    return decorate\n",
    "\n",
    "\"\"\"Iterate through all methods in a class and apply decorator.\n",
    "THIS VERSION DOES WORK FOR INHERITED METHODS AS WELL BUT REALLY DOES FOR ALL OF THEM\"\"\"\n",
    "def decorate_all_methods_v2(decorator, exclude=None):\n",
    "    if exclude is None:\n",
    "        exclude = []\n",
    "\n",
    "    def decorate(cls):\n",
    "        for name, fn in inspect.getmembers(cls, inspect.isroutine):\n",
    "            if name not in exclude:\n",
    "                setattr(cls, name, decorator(fn))\n",
    "        return cls\n",
    "\n",
    "    return decorate\n",
    "\n",
    "allowed_hooks = [\"on_after_backward\",\"on_after_batch_transfer\",\"on_before_backward\",\"on_before_batch_transfer\",\"on_before_optimizer_step\",\"on_before_zero_grad\",\"on_fit_end\",\"on_fit_start\",\"on_load_checkpoint\",\"on_predict_batch_end\",\"on_predict_batch_start\",\"on_predict_end\",\"on_predict_epoch_end\",\"on_predict_epoch_start\",\"on_predict_model_eval\",\"on_predict_start\",\"on_save_checkpoint\",\"on_test_batch_end\",\"on_test_batch_start\",\"on_test_end\",\"on_test_epoch_end\",\"on_test_epoch_start\",\"on_test_model_eval\",\"on_test_model_train\",\"on_test_start\",\"on_train_batch_end\",\"on_train_batch_start\",\"on_train_end\",\"on_train_epoch_end\",\"on_train_epoch_start\",\"on_train_start\",\"on_validation_batch_end\",\"on_validation_batch_start\",\"on_validation_end\",\"on_validation_epoch_end\",\"on_validation_epoch_start\",\"on_validation_model_eval\",\"on_validation_model_train\",\"on_validation_start\"]\n",
    "allowed_hooks = [\"on_predict_batch_end\",\"on_predict_batch_start\",\"on_predict_end\",\"on_predict_epoch_end\",\"on_predict_epoch_start\",\"on_predict_start\",\"on_test_batch_end\",\"on_test_batch_start\",\"on_test_end\",\"on_test_epoch_end\",\"on_test_epoch_start\",\"on_test_start\",\"on_train_batch_end\",\"on_train_batch_start\",\"on_train_end\",\"on_train_epoch_end\",\"on_train_epoch_start\",\"on_train_start\",\"on_validation_batch_end\",\"on_validation_batch_start\",\"on_validation_end\",\"on_validation_epoch_end\",\"on_validation_epoch_start\",\"on_validation_start\"]\n",
    "\n",
    "\"\"\"Iterate through a list of methods in a class and apply decorator.\n",
    "THIS VERSION DOES WORK FOR INHERITED METHODS AS WELL\"\"\"\n",
    "def decorate_methods(decorator, methods=allowed_hooks):\n",
    "    def decorate(cls):\n",
    "        for name, fn in inspect.getmembers(cls, inspect.isroutine):\n",
    "            if name in methods :\n",
    "                setattr(cls, name, decorator(fn))\n",
    "        return cls\n",
    "\n",
    "    return decorate\n",
    "\n",
    "#@decorate_all_methods_v2(apply_hooks, exclude=[\"__init__\"])\n",
    "@decorate_methods(apply_hooks,methods=allowed_hooks)\n",
    "class Normalization(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_in, mode = 'std', hooks = {'on_train_epoch_end': 'save_stats', 'on_train_epoch_start': 'reset_stats' }):\n",
    "        super().__init__()\n",
    "        \n",
    "        # buffers containing mean and range for standardization\n",
    "        self.register_buffer(\"Mean\", torch.zeros(n_in))\n",
    "        self.register_buffer(\"Range\", torch.ones(n_in))\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_in\n",
    "\n",
    "        if mode == 'std':\n",
    "            self.running_stats = RunningAverageStd()\n",
    "        elif mode == 'minmax':\n",
    "            self.running_stats = RunningMinMax()\n",
    "        else:\n",
    "            raise ValueError(f'The normalization mode should be one of the following: \"std\", \"minmax\", not {mode}.')\n",
    "\n",
    "        self.hooks = hooks\n",
    "\n",
    "    def reset_stats(self) -> None:\n",
    "        self.running_stats.reset()\n",
    "\n",
    "    def save_stats(self) -> None:\n",
    "        self.Mean = self.running_stats.mean\n",
    "        self.Range = self.running_stats.range\n",
    "\n",
    "    def get_mean_range(self, size : torch.Size) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"Return mean and range reshaped according to tensor size. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : torch.Size\n",
    "                \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _type_\n",
    "            _description_\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            _description_\n",
    "        \"\"\"\n",
    "        if len(size) == 2:\n",
    "            batch_size = size[0]\n",
    "            x_size = size[1]\n",
    "            Mean = self.Mean.unsqueeze(0).expand(batch_size, x_size)\n",
    "            Range = self.Range.unsqueeze(0).expand(batch_size, x_size)\n",
    "        elif len(size) == 1:\n",
    "            Mean = self.Mean\n",
    "            Range = self.Range\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Input tensor must of shape (n_features) or (n_batch,n_features), not {size}.\"\n",
    "            )\n",
    "        return Mean,Range\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> (torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute standardized inputs/outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            input/output\n",
    "        Mean: torch.Tensor\n",
    "            mean values tostart be subtracted.\n",
    "        Range: torch.Tensor\n",
    "            interval range to be divided by.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor\n",
    "            standardized inputs/outputs\n",
    "        \"\"\"\n",
    "\n",
    "        # accumulate statistics during training\n",
    "        if self.training:\n",
    "            self.running_stats(x)\n",
    "        # get mean and range\n",
    "        Mean,Range = self.get_mean_range(x.size())\n",
    "    \n",
    "        return x.sub(Mean).div(Range)\n",
    "\n",
    "    def inverse(self, x: torch.Tensor) -> (torch.Tensor):\n",
    "        \"\"\"\n",
    "        Remove standardization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            input/output\n",
    "        Mean: torch.Tensor\n",
    "            mean values to be added back.\n",
    "        Range: torch.Tensor\n",
    "            interval range to be multiplied back.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor\n",
    "            standardized inputs/outputs\n",
    "        \"\"\"\n",
    "        # get mean and range\n",
    "        Mean,Range = self.get_mean_range(x.size())\n",
    "\n",
    "        return x.mul(Range).add(Mean)\n",
    "\n",
    "def test_normalization():\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    n_in = 2\n",
    "    norm = Normalization(n_in, mode='std')\n",
    "\n",
    "    X = torch.randn((5,n_in))*10\n",
    "\n",
    "    # accumulate stats during training\n",
    "    print('TRAIN')\n",
    "    norm.train()\n",
    "    # reset variables \n",
    "    norm.on_train_epoch_start()\n",
    "    #norm.reset_stats()\n",
    "    # propagate X and accumulate\n",
    "    print(norm(X))\n",
    "    # display current stats\n",
    "    print('running stats')\n",
    "    print(norm.running_stats)\n",
    "    # save running stats for inference\n",
    "    norm.on_train_epoch_end()\n",
    "    #norm.save_stats()\n",
    "    print('saved stats')\n",
    "    print(norm.Mean,norm.Range)\n",
    "\n",
    "    # Then use the estimate for predict\n",
    "    print('EVAL')\n",
    "    norm.eval()\n",
    "    print('Normalize')\n",
    "    print(norm(X))\n",
    "    print('Un-normalize')\n",
    "    print(norm.inverse(norm(X)))\n",
    "\n",
    "test_normalization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "tensor([[  3.3669,   1.2881],\n",
      "        [  2.3446,   2.3033],\n",
      "        [-11.2286,  -1.8633],\n",
      "        [ 22.0820,  -6.3800],\n",
      "        [  4.6166,   2.6735]])\n",
      "running stats\n",
      "<Running average +- std: tensor([ 4.2363, -0.3957]) +- tensor([11.8544,  3.7916])>\n",
      "saved stats\n",
      "tensor([ 4.2363, -0.3957]) tensor([11.8544,  3.7916])\n",
      "EVAL\n",
      "Normalize\n",
      "tensor([[-0.0733,  0.4441],\n",
      "        [-0.1596,  0.7118],\n",
      "        [-1.3046, -0.3871],\n",
      "        [ 1.5054, -1.5783],\n",
      "        [ 0.0321,  0.8095]])\n",
      "Un-normalize\n",
      "tensor([[  3.3669,   1.2881],\n",
      "        [  2.3446,   2.3033],\n",
      "        [-11.2286,  -1.8633],\n",
      "        [ 22.0820,  -6.3800],\n",
      "        [  4.6166,   2.6735]])\n",
      "-------------\n",
      "input\n",
      "tensor([[  1., 100.],\n",
      "        [ 10.,   2.],\n",
      "        [  5.,  50.]])\n",
      "min tensor([1., 2.])\n",
      "max tensor([ 10., 100.])\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import functools\n",
    "\n",
    "def apply_hooks(f):\n",
    "    @functools.wraps(f)\n",
    "    def wrapper(self,*args, **kwargs):\n",
    "        # loop over all saved hooks in class\n",
    "        for hook,funcs in self.hooks.items():\n",
    "            # check if the current function name matches a saved hook\n",
    "            # in case execute all the associated funcs\n",
    "            if hook == f.__name__:\n",
    "                # if it is a string convert to iterable\n",
    "                if isinstance(funcs, str):\n",
    "                    funcs = [ funcs ] \n",
    "                for func in funcs:\n",
    "                    getattr(self,func)()\n",
    "        return f(self,*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "allowed_hooks = [\"on_predict_batch_end\",\"on_predict_batch_start\",\"on_predict_end\",\"on_predict_epoch_end\",\"on_predict_epoch_start\",\"on_predict_start\",\"on_test_batch_end\",\"on_test_batch_start\",\"on_test_end\",\"on_test_epoch_end\",\"on_test_epoch_start\",\"on_test_start\",\"on_train_batch_end\",\"on_train_batch_start\",\"on_train_end\",\"on_train_epoch_end\",\"on_train_epoch_start\",\"on_train_start\",\"on_validation_batch_end\",\"on_validation_batch_start\",\"on_validation_end\",\"on_validation_epoch_end\",\"on_validation_epoch_start\",\"on_validation_start\"]\n",
    "\n",
    "\"\"\"Iterate through a list of methods in a class and apply decorator.\"\"\"\n",
    "def decorate_methods(decorator, methods=allowed_hooks):\n",
    "    def decorate(cls):\n",
    "        for name, fn in inspect.getmembers(cls, inspect.isroutine):\n",
    "            if name in methods :\n",
    "                setattr(cls, name, decorator(fn))\n",
    "        return cls\n",
    "\n",
    "    return decorate\n",
    "\n",
    "@decorate_methods(apply_hooks,methods=allowed_hooks)\n",
    "class Normalization(pl.LightningModule):\n",
    "    \"\"\"_summary_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, mode = 'std', hooks = {'on_train_epoch_end': 'save_stats', 'on_train_epoch_start': 'reset_stats' }):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # buffers containing mean and range for standardization\n",
    "        self.register_buffer(\"Mean\", torch.zeros(n_in))\n",
    "        self.register_buffer(\"Range\", torch.ones(n_in))\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_in\n",
    "\n",
    "        if mode == 'std':\n",
    "            self.running_stats = RunningAverageStd()\n",
    "        elif mode == 'minmax':\n",
    "            self.running_stats = RunningMinMax()\n",
    "        else:\n",
    "            raise ValueError(f'The normalization mode should be one of the following: \"std\", \"minmax\", not {mode}.')\n",
    "\n",
    "        self.hooks = hooks\n",
    "\n",
    "    def reset_stats(self) -> None:\n",
    "        \"\"\"Reset running statistics.\"\"\"\n",
    "        self.running_stats.reset()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def save_stats(self) -> None:\n",
    "        \"\"\"Save running statistics into class methods for later inference.\"\"\"\n",
    "        self.Mean = self.running_stats.mean.detach()\n",
    "        self.Range = self.running_stats.range.detach()\n",
    "\n",
    "    def get_mean_range(self, size : torch.Size) -> tuple[torch.Tensor]:\n",
    "        \"\"\"Return mean and range reshaped according to tensor size. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : torch.Size\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (Mean, Range): Tuple[torch.Tensor]\n",
    "            Mean and range \n",
    "\n",
    "        \"\"\"\n",
    "        if len(size) == 2:\n",
    "            batch_size = size[0]\n",
    "            x_size = size[1]\n",
    "            Mean = self.Mean.unsqueeze(0).expand(batch_size, x_size)\n",
    "            Range = self.Range.unsqueeze(0).expand(batch_size, x_size)\n",
    "        elif len(size) == 1:\n",
    "            Mean = self.Mean\n",
    "            Range = self.Range\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Input tensor must of shape (n_features) or (n_batch,n_features), not {size}.\"\n",
    "            )\n",
    "        return (Mean,Range)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> (torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute standardized inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            input/output\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor\n",
    "            standardized inputs\n",
    "        \"\"\"\n",
    "        # accumulate statistics during training\n",
    "        if self.training:\n",
    "            self.running_stats(x)\n",
    "        # get mean and range\n",
    "        Mean,Range = self.get_mean_range(x.size())\n",
    "    \n",
    "        return x.sub(Mean).div(Range)\n",
    "\n",
    "    def inverse(self, x: torch.Tensor) -> (torch.Tensor):\n",
    "        \"\"\"\n",
    "        Remove standardization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            input\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor\n",
    "            un-normalized inputs\n",
    "        \"\"\"\n",
    "        # get mean and range\n",
    "        Mean,Range = self.get_mean_range(x.size())\n",
    "\n",
    "        return x.mul(Range).add(Mean)\n",
    "\n",
    "def test_normalization():\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    n_in = 2\n",
    "    norm = Normalization(n_in, mode='std')\n",
    "\n",
    "    X = torch.randn((5,n_in))*10\n",
    "\n",
    "    # accumulate stats during training\n",
    "    print('TRAIN')\n",
    "    norm.train()\n",
    "    # reset variables \n",
    "    norm.on_train_epoch_start()\n",
    "    #norm.reset_stats()\n",
    "    # propagate X and accumulate\n",
    "    print(norm(X))\n",
    "    # display current stats\n",
    "    print('running stats')\n",
    "    print(norm.running_stats)\n",
    "    # save running stats for inference\n",
    "    norm.on_train_epoch_end()\n",
    "    #norm.save_stats()\n",
    "    print('saved stats')\n",
    "    print(norm.Mean,norm.Range)\n",
    "\n",
    "    # Then use the estimate for predict\n",
    "    print('EVAL')\n",
    "    norm.eval()\n",
    "    print('Normalize')\n",
    "    print(norm(X))\n",
    "    print('Un-normalize')\n",
    "    print(norm.inverse(norm(X)))\n",
    "    print('-------------')\n",
    "    # check other way of normalizing\n",
    "    norm = Normalization(n_in, mode='minmax')\n",
    "    norm.train()\n",
    "    X = torch.Tensor([[1,100,],[10,2],[5,50]])\n",
    "    print('input')\n",
    "    print(X)\n",
    "    norm(X)\n",
    "    norm.save_stats()\n",
    "    print('min', norm.running_stats.Min)\n",
    "    print('max', norm.running_stats.Max)\n",
    "    assert torch.allclose( norm.running_stats.Min, X.min(0)[0])\n",
    "    assert torch.allclose( norm.running_stats.Max, X.max(0)[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_normalization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV example class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "from typing import Any\n",
    "\n",
    "def call_submodules_hooks(f):\n",
    "    @functools.wraps(f)\n",
    "    def wrapper(self,*args, **kwargs):\n",
    "        for child in self.children():\n",
    "            if isinstance(child,pl.LightningModule):\n",
    "                func = getattr(child,f.__name__)\n",
    "                func(*args, **kwargs)\n",
    "        return f(self,*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "allowed_hooks = [\"on_after_backward\",\"on_after_batch_transfer\",\"on_before_backward\",\"on_before_batch_transfer\",\"on_before_optimizer_step\",\"on_before_zero_grad\",\"on_fit_end\",\"on_fit_start\",\"on_load_checkpoint\",\"on_predict_batch_end\",\"on_predict_batch_start\",\"on_predict_end\",\"on_predict_epoch_end\",\"on_predict_epoch_start\",\"on_predict_model_eval\",\"on_predict_start\",\"on_save_checkpoint\",\"on_test_batch_end\",\"on_test_batch_start\",\"on_test_end\",\"on_test_epoch_end\",\"on_test_epoch_start\",\"on_test_model_eval\",\"on_test_model_train\",\"on_test_start\",\"on_train_batch_end\",\"on_train_batch_start\",\"on_train_end\",\"on_train_epoch_end\",\"on_train_epoch_start\",\"on_train_start\",\"on_validation_batch_end\",\"on_validation_batch_start\",\"on_validation_end\",\"on_validation_epoch_end\",\"on_validation_epoch_start\",\"on_validation_model_eval\",\"on_validation_model_train\",\"on_validation_start\"]\n",
    "allowed_hooks = [\"on_predict_batch_end\",\"on_predict_batch_start\",\"on_predict_end\",\"on_predict_epoch_end\",\"on_predict_epoch_start\",\"on_predict_start\",\"on_test_batch_end\",\"on_test_batch_start\",\"on_test_end\",\"on_test_epoch_end\",\"on_test_epoch_start\",\"on_test_start\",\"on_train_batch_end\",\"on_train_batch_start\",\"on_train_end\",\"on_train_epoch_end\",\"on_train_epoch_start\",\"on_train_start\",\"on_validation_batch_end\",\"on_validation_batch_start\",\"on_validation_end\",\"on_validation_epoch_end\",\"on_validation_epoch_start\",\"on_validation_start\"]\n",
    "\n",
    "\"\"\"Iterate through a list of methods in a class and apply decorator.\"\"\"\n",
    "def decorate_methods(decorator, methods=allowed_hooks):\n",
    "    def decorate(cls):\n",
    "        for name, fn in inspect.getmembers(cls, inspect.isroutine):\n",
    "            if name in methods :\n",
    "                setattr(cls, name, decorator(fn))\n",
    "        return cls\n",
    "\n",
    "    return decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | In sizes | Out sizes\n",
      "-------------------------------------------------------------\n",
      "0 | blocks | Sequential | 86     | [2]      | [1]      \n",
      "-------------------------------------------------------------\n",
      "86        Trainable params\n",
      "0         Non-trainable params\n",
      "86        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCV(\n",
      "  (blocks): Sequential(\n",
      "    (0): Normalization()\n",
      "    (1): FeedForward(\n",
      "      (nn): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Linear(in_features=10, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------\n",
      "SimpleCV(\n",
      "  (blocks): Sequential(\n",
      "    (0): Normalization()\n",
      "    (1): FeedForward(\n",
      "      (nn): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=10, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch 1: 100%|██████████| 4/4 [00:00<00:00, 288.13it/s, loss=8.75]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4/4 [00:00<00:00, 249.26it/s, loss=8.75]\n"
     ]
    }
   ],
   "source": [
    "@decorate_methods(call_submodules_hooks,methods=allowed_hooks)\n",
    "class SimpleCV(pl.LightningModule):\n",
    "    \"\"\"Example of collective variable obtained with a regression task.\n",
    "    Initalize the architecture as a torch.nn.sequential model and the \n",
    "    loss function (MSE) in the training_step function. \"\"\"\n",
    "    \n",
    "    def __init__(self, options : dict[str, Any] = None , modules : list = None, **kwargs):\n",
    "        \"\"\"Initialize the CV either from a dictionary or with a list of pl.LightningModules for greater flexibility. \n",
    "\n",
    "        If instead a list of modules is given they will supersede the options dict.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        options : dict[str, Any], optional\n",
    "            The allowed keys are :\n",
    "            - layers  :  list of nodes per layers (e.g. [n_input, n_hidden, n_output]).\n",
    "            - activation : type of activation function (see FeedForward class) \n",
    "            - normIn  : how to normalize inputs (std,minmax,False/None), default std\n",
    "        modules : list of pl.LightningModules, alternative\n",
    "            list of modules, by default None\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if ( modules is not None ):\n",
    "            self.blocks = torch.nn.Sequential(*modules)\n",
    "        else:\n",
    "            # parse layers\n",
    "            try:\n",
    "                layers = options['layers']\n",
    "            except KeyError:\n",
    "                raise KeyError('the key layers (e.g. [n_input, n_hidden, n_output]) is missing in the options dict')\n",
    "            n_in = layers[0] \n",
    "            # parse activation\n",
    "            activation = options['activation'] if 'activation' in options else 'relu'\n",
    "            # create NN\n",
    "            nn = FeedForward(layers, activation=activation)\n",
    "            # norm inputs\n",
    "            modules = []\n",
    "            if ( 'normIn' in options ):\n",
    "                normIn = options['normIn']\n",
    "                if (normIn != False) and (normIn is not None):\n",
    "                    if normIn == True:\n",
    "                        normIn = 'std'\n",
    "                    mode = normIn\n",
    "                    modules.append(Normalization(n_in,mode=normIn))\n",
    "            modules.append(nn)\n",
    "                       \n",
    "            self.blocks = torch.nn.Sequential(*modules)\n",
    "\n",
    "        self.example_input_array = torch.ones(modules[0].n_in)\n",
    "\n",
    "        # parameters\n",
    "        self.n_in = modules[0].n_in\n",
    "        self.n_out = modules[-1].n_out\n",
    "        self.lr = 1e-3\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> (torch.tensor):\n",
    "        y = self.blocks(x)\n",
    "        return y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def loss_function(self, input, target):\n",
    "        # MSE LOSS\n",
    "        loss = (input-target).square().mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, labels = train_batch\n",
    "        y = self(x)\n",
    "        loss = self.loss_function(y,labels)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, labels = val_batch\n",
    "        y = self(x)\n",
    "        loss = self.loss_function(y,labels)\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "\n",
    "def test_simplecv():\n",
    "    n_in, n_out = 2,1 \n",
    "    layers = [n_in, 5, 10, n_out]\n",
    "\n",
    "    # initialize via dictionary\n",
    "    options= { 'layers' : layers ,\n",
    "               'activation' : 'relu',\n",
    "               'normIn' : True }    \n",
    "    model = SimpleCV( options )\n",
    "    print(model)\n",
    "    print('----------')\n",
    "\n",
    "    # initialize via list of modules\n",
    "    modules = [ Normalization(n_in),\n",
    "                FeedForward(layers, activation='tanh') ]\n",
    "    model = SimpleCV( modules=modules )\n",
    "    print(model)\n",
    "\n",
    "    # create dataset\n",
    "    X = torch.randn((100,2))\n",
    "    y = X.square().sum(1)\n",
    "    dataset = TensorDataset(X,y)\n",
    "    datamodule = TensorDataModule(dataset,lengths=[0.75,0.2,0.05], batch_size=25)\n",
    "    # train model\n",
    "    trainer = pl.Trainer(accelerator='cpu',max_epochs=2,logger=None)\n",
    "    trainer.fit( model, datamodule )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_simplecv() "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_in = 2 \n",
    "X = torch.ones(n_in)\n",
    "\n",
    "layers = [n_in, 10, 1]\n",
    "\n",
    "params = {  'layers': layers,\n",
    "            'activation': 'tanh',\n",
    "            'normIn': True,\n",
    "            'normOut' : False,\n",
    "         }\n",
    "\n",
    "model = SimpleCV( **params )\n",
    "\n",
    "s = model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_points = 10000\n",
    "\n",
    "n_in = 1\n",
    "n_out = 1\n",
    "\n",
    "X = torch.randn((n_points,n_in))*2\n",
    "y = X.square()\n",
    "\n",
    "dataset = TensorDataset(X,y)\n",
    "train_dataset, valid_dataset = random_split(dataset, [8000,2000])\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=200)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 2]) torch.Size([25, 2])\n",
      "torch.Size([25, 2]) torch.Size([25, 2])\n",
      "torch.Size([25, 2]) torch.Size([25, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from typing import Union\n",
    "\n",
    "class TensorDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Lightning DataModule constructed for TensorDataset(s).\"\"\"\n",
    "    def __init__(self, dataset: TensorDataset, lengths=[0.8,0.2,0], batch_size: Union[int,list] = 32, shuffle : Union[bool,list] =  False,  generator : torch.Generator = None):\n",
    "        \"\"\"Create a DataModule derived from TensorDataset, which returns train/valid/test dataloaders.\n",
    "\n",
    "        For the batch_size and shuffle parameters either a single value or a list-type of values (with same size as lenghts) can be provided.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : TensorDataset\n",
    "            Train dataset\n",
    "        lengths : list, optional\n",
    "            Lenghts of the training/validation/test datasets , by default [0.8,0.2]\n",
    "        batch_size : int or list, optional\n",
    "            Batch size, by default 32\n",
    "        shuffle : Union[bool,list], optional\n",
    "            whether to shuffle the batches from the dataloader, by default False\n",
    "        generator : torch.Generator, optional\n",
    "            set random generator for reproducibility, by default None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.lengths = lengths\n",
    "        if isinstance(batch_size,int):\n",
    "            self.batch_size = [batch_size for _ in lengths ]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        if isinstance(shuffle,bool):\n",
    "            self.shuffle = [shuffle for _ in lengths ]\n",
    "        else:\n",
    "            self.shuffle = shuffle\n",
    "        self.generator = None\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.dataset_splits = random_split(self.dataset, self.lengths, generator=self.generator)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Return training dataloader.\"\"\"\n",
    "        return FastTensorDataLoader(self.dataset_splits[0], batch_size=self.batch_size[0],shuffle=self.shuffle[0])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Return validation dataloader.\"\"\"\n",
    "        return FastTensorDataLoader(self.dataset_splits[1], batch_size=self.batch_size[1],shuffle=self.shuffle[1])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Return test dataloader.\"\"\"\n",
    "        if len(self.lengths) >= 3:\n",
    "            return DataLoader(self.dataset_splits[2], batch_size=self.batch_size[2],shuffle=self.shuffle[2])\n",
    "        else: \n",
    "            raise ValueError('Test dataset not available, you need to pass three lenghts to datamodule.')  \n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def teardown(self, stage: str):\n",
    "        pass \n",
    "\n",
    "def test_tensordatamodule():\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn((100,2))\n",
    "    y = X.square()\n",
    "    dataset = TensorDataset(X,y)\n",
    "\n",
    "    datamodule = TensorDataModule(dataset,lengths=[0.75,0.2,0.05],batch_size=25)\n",
    "    datamodule.setup('fit')\n",
    "    loader = datamodule.train_dataloader()\n",
    "    for data in loader:\n",
    "        x_i, y_i = data\n",
    "        print(x_i.shape, y_i.shape)\n",
    "    datamodule.val_dataloader()\n",
    "    datamodule.test_dataloader()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tensordatamodule() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_points = 1000\n",
    "\n",
    "n_in = 1\n",
    "n_out = 1\n",
    "\n",
    "X = torch.randn((n_points,n_in))*2\n",
    "y = X.square()\n",
    "\n",
    "dataset = TensorDataset(X,y)\n",
    "\n",
    "datamodule = TensorDataModule(dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model and forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCV(\n",
      "  (blocks): Sequential(\n",
      "    (0): Normalization()\n",
      "    (1): FeedForward(\n",
      "      (nn): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "        (6): Linear(in_features=10, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "  | Name          | Type          | Params | In sizes | Out sizes\n",
      "-----------------------------------------------------------------------\n",
      "0 | blocks        | Sequential    | 461    | [1]      | [1]      \n",
      "1 | blocks.0      | Normalization | 0      | [1]      | [1]      \n",
      "2 | blocks.1      | FeedForward   | 461    | [1]      | [1]      \n",
      "3 | blocks.1.nn   | Sequential    | 461    | [1]      | [1]      \n",
      "4 | blocks.1.nn.0 | Linear        | 20     | [1]      | [10]     \n",
      "5 | blocks.1.nn.1 | ReLU          | 0      | [10]     | [10]     \n",
      "6 | blocks.1.nn.2 | Linear        | 220    | [10]     | [20]     \n",
      "7 | blocks.1.nn.4 | Linear        | 210    | [20]     | [10]     \n",
      "8 | blocks.1.nn.6 | Linear        | 11     | [10]     | [1]      \n",
      "-----------------------------------------------------------------------\n",
      "461       Trainable params\n",
      "0         Non-trainable params\n",
      "461       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "\n",
    "layers = [n_in, 10, 20, 10, n_out]\n",
    "\n",
    "modules = [ Normalization(n_in),\n",
    "            FeedForward(layers,activation='relu') ]\n",
    "\n",
    "model = SimpleCV( modules = modules )\n",
    "\n",
    "print(model)\n",
    "summary = ModelSummary(model, max_depth=-1)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lbonati@iit.local/software/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataset.py:342: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params | In sizes | Out sizes\n",
      "-------------------------------------------------------------\n",
      "0 | blocks | Sequential | 461    | [1]      | [1]      \n",
      "-------------------------------------------------------------\n",
      "461       Trainable params\n",
      "0         Non-trainable params\n",
      "461       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 32/32 [00:00<00:00, 138.45it/s, loss=0.397, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 32/32 [00:00<00:00, 135.12it/s, loss=0.397, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger,TensorBoardLogger\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, \n",
    "                        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=0.1, patience=10, verbose=False)],\n",
    "                        max_epochs=50, log_every_n_steps=1,logger=[TensorBoardLogger(\"logs2/\"), CSVLogger(\"logs\", name=\"enrico\") ])\n",
    "\n",
    "trainer.fit( model, datamodule )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0].Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "plt.scatter(X, model(X).detach().numpy(),s=1)\n",
    "plt.scatter(X, y, s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoderCV(\n",
      "  (encoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=6, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=6, out_features=4, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Linear(in_features=4, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=6, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Linear(in_features=6, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from typing import Any,Union\n",
    "\n",
    "@decorate_methods(call_submodules_hooks, methods=allowed_hooks)\n",
    "class AutoEncoderCV(pl.LightningModule):\n",
    "    \"\"\"AutoEncoding Collective Variable.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_layers : list , decoder_layers : list = None, options : dict[str,Any] = {} ):\n",
    "        \"\"\"TODO \n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Members\n",
    "        blocks = ['normIn','encoder','normOut','decoder'] \n",
    "\n",
    "        # Initialize defaults\n",
    "        for b in blocks:\n",
    "            self.__setattr__(b,None)\n",
    "            options.setdefault(b,{})\n",
    "\n",
    "        # parse info from args\n",
    "        if decoder_layers is None:\n",
    "            decoder_layers = encoder_layers[::-1]\n",
    "        self.n_in = layers[0]\n",
    "        self.n_out = layers[-1]\n",
    "\n",
    "        # initialize normIn\n",
    "        o = 'normIn'\n",
    "        if ( not options[o] ) and (options[o] is not None):\n",
    "            self.normIn = Normalization(self.n_in,**options[o]) \n",
    "\n",
    "        # initialize encoder\n",
    "        o = 'encoder'\n",
    "        self.encoder = FeedForward(encoder_layers, **options[o])\n",
    "\n",
    "         # initialize normOut\n",
    "        o = 'normOut'\n",
    "        if ( not options[o] ) and (options[o] is not None):\n",
    "            self.normOut = Normalization(self.n_out,**options[o]) \n",
    "\n",
    "        # initialize encoder\n",
    "        o = 'decoder'\n",
    "        self.decoder = FeedForward(decoder_layers, **options[o])\n",
    "\n",
    "        # set input example\n",
    "        self.example_input_array = torch.ones(self.n_in)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> (torch.tensor):\n",
    "        if self.normIn is not None:\n",
    "            x = self.normIn(x)\n",
    "        x = self.encoder(x)\n",
    "        if self.normOut is not None:\n",
    "            x = self.normOut(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def encode_decode(self, x: torch.tensor) -> (torch.tensor):\n",
    "        x = self.forward(x)\n",
    "        x = self.decoder(x)\n",
    "        if self.normIn is not None:\n",
    "            x = self.normIn.inverse(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def loss_function(self, input, target):\n",
    "        # Reconstruction (MSE) loss\n",
    "        loss = (input-target).square().mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x = train_batch[0]\n",
    "        x_hat = self.encode_decode(x)\n",
    "        loss = self.loss_function(x_hat,x)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x = val_batch[0]\n",
    "        x_hat = self.encode_decode(x)\n",
    "        loss = self.loss_function(x_hat,x)\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "\n",
    "def test_autoencodercv():\n",
    "    n_in, n_out = 8,2\n",
    "    layers = [n_in, 6, 4, n_out]\n",
    "\n",
    "    # initialize via dictionary\n",
    "    opts = { 'normIn'  : None,\n",
    "             'encoder' : { 'activation' : 'relu' },\n",
    "             'normOut' : { 'mode'   : 'std' },\n",
    "           } \n",
    "    model = AutoEncoderCV( encoder_layers=layers, options=opts )\n",
    "    print(model)\n",
    "    print('----------')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_autoencodercv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_out = 2,1\n",
    "\n",
    "blocks = ['normIn','encoder','normOut','decoder'] \n",
    "\n",
    "for b in blocks:\n",
    "    options.setdefault(b,{})\n",
    "\n",
    "layers = [n_in, 10, 20, 10, n_out]\n",
    "\n",
    "options = { 'normIn'  : None,\n",
    "            'encoder' : { 'activation' : 'relu' },\n",
    "            'normOut' : { 'mode'   : 'std' },\n",
    "            'decoder' : {  }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normIn': None,\n",
       " 'encoder': {'activation': 'relu'},\n",
       " 'normOut': {'mode': 'std'},\n",
       " 'decoder': {}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_in, n_out = 2,1\n",
    "layers = [n_in, 10, 20, 10, n_out]\n",
    "\n",
    "options = { 'normIn'  : None,\n",
    "            'encoder' : { 'activation' : 'relu' },\n",
    "            'normOut' : { 'mode'   : 'std' },\n",
    "          }\n",
    "\n",
    "blocks = ['normIn','encoder','normOut','decoder'] \n",
    "\n",
    "for b in blocks:\n",
    "    options.setdefault(b,{})\n",
    "\n",
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_out = 2,1\n",
    "layers = [n_in, 10, 20, 10, n_out]\n",
    "\n",
    "options= { 'encoder' : layers ,\n",
    "            'activation' : 'relu',\n",
    "            'normIn' : True, \n",
    "            'normOut' : 'minmax' }    \n",
    "model = AutoEncoderCV( options )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lbonati@iit.local/software/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataset.py:342: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params | In sizes | Out sizes\n",
      "-----------------------------------------------------------------\n",
      "0 | normIn  | Normalization | 0      | [2]      | [2]      \n",
      "1 | encoder | FeedForward   | 471    | [2]      | [1]      \n",
      "2 | normOut | Normalization | 0      | [1]      | [1]      \n",
      "3 | decoder | FeedForward   | 472    | ?        | ?        \n",
      "-----------------------------------------------------------------\n",
      "943       Trainable params\n",
      "0         Non-trainable params\n",
      "943       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 32/32 [00:00<00:00, 205.75it/s, loss=4.77e+03, v_num=4]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "X = torch.randn((1000,n_in))*100\n",
    "\n",
    "dataset = TensorDataset(X)\n",
    "\n",
    "datamodule = TensorDataModule(dataset,batch_size=32)\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, \n",
    "                        callbacks=[EarlyStopping(monitor=\"val_loss\")],\n",
    "                        max_epochs=100, log_every_n_steps=1)\n",
    "\n",
    "trainer.fit( model, datamodule )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval() \n",
    "X_hat = model.encode_decode(X).detach().numpy()\n",
    "s = model(X).detach().numpy()\n",
    "plt.figure()\n",
    "plt.scatter(X[:,1],X_hat[:,1])\n",
    "plt.figure()\n",
    "#plt.hist((X-X_hat).square().sum(1).sqrt().div(X.square().sum(1).sqrt()).numpy(),bins=100)#,(X-X_hat)[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to_torchscript(method=\"trace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4775], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cbeac1d7079eaeba64f3210ccac5ee24400128e300a45ae35eee837885b08b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
